#!/usr/bin/env python
# coding: utf-8

# # [Adult Dataset](https://bit.ly/3rvRB8j)
# 
# ### Celem jest zrobiÄ‡ predykcjÄ™, czy dana osoba osiÄ…gnie przychÃ³d ponad 50 tysiÄ™cy (dolarÃ³w) w rok.
# 
# Innymi sÅ‚owy, z punktu widzenia uczenie maszynowego naleÅ¼y wykonaÄ‡ klasyfikacjÄ™ binarnÄ…. W praktyce to oznacza, Å¼e sÄ… dwie moÅ¼liwe odpowiedzi: np. "tak" czy "nie" lub "kot" czy "pies" itd.
# 

# ### Krok po kroku 
# 
# JeÅ›li wolisz najpierw sÅ‚uchaÄ‡ i oglÄ…daÄ‡, to obejrzyj nagranie poniÅ¼ej, ktÃ³re omawia tÄ™ lekcjÄ™. 

# In[1]:


get_ipython().run_cell_magic('html', '', '<iframe style="height:500px;width:100%" src="https://bit.ly/31v8THY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>')


# In[1]:


import pandas as pd
import numpy as np

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

#models
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.preprocessing import LabelEncoder

import qgrid

import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')


# ## Wczytujemy dane
# Dane sÄ… przechowywane w formacie [HDF5](https://bit.ly/3w6jbwk). To jest binarny format, ktÃ³ry jest doÅ›Ä‡ wygodny (zwykle trzymamy dane w tym formacie zamiast .csv). MiÄ™dzy innymi umoÅ¼liwia to zapisanie wiÄ™cej niÅ¼ jeden zbiÃ³r danych do jednego pliku.

# In[2]:


train = pd.read_hdf('../input/train.adult.h5')


# ## Pytania 1.5.1
# O zbiorze danych:
# 1. Ile jest obiektÃ³w (wierszy)?
# 2. Ile jest cech/features (column)?
# 3. Ile jest potrzebnej pamiÄ™ci?
# 4. Jakie sÄ… dane?
#     4.1 Czy sÄ… zmienne **iloÅ›ciowe** lub **ciÄ…gÅ‚e** (takie jak wiek, wzrost, waga...) i jak wyglÄ…da ich rozkÅ‚ad?
#     4.2 Czy sÄ… zmienne **nominalne** (i jak duÅ¼o unikalnych wartoÅ›ci ma kaÅ¼da z nich)?
#     4.3 Czy sÄ… zmienne **porzÄ…dkowe** (np. bardzo dobry, dobry, zÅ‚y)?
#     4.4 Czy sÄ… daty?
# 5. Co jest **zmiennÄ… docelowÄ…** (ang. *target variable*)?
#     5.1 Jakiego typu jest ta zmienna?
#     5.2 Czy sÄ… jakieÅ› zmienne, ktÃ³re sÄ… mocno skorelowane ze zmiennÄ… docelowÄ…?
# 6. Jak duÅ¼o zmiennych ma brak danych?
# 7. Czy jest zauwaÅ¼alna [(liniowa) korelacja](https://bit.ly/3lZa1gq) pomiÄ™dzy cechami (w szczegÃ³lnoÅ›ci powiÄ…zane ze zmiennÄ… docelowÄ…)?

# ## Odpowiedzi/wnioski 1.5.1
# 
# 1.  32561
# 2.  15
# 3.  2.9+MB
# 4.  Dane opisujÄ… cechy ludzi: pÅ‚eÄ‡,wyksztaÅ‚cenie,zawÃ³d, stopieÅ„ edukacji, iloÅ›Ä‡ godzin pracy, kraj pochodzenia itp.    
#     4.1  Numerical fetures: (Age, fnlwgt, Capital Gain, Capital Loss, Hours per week) 
#     4.2  Categorical fetures: (Workclass, Martial Status, Occupation, Relationship, Race, Sex, Country, Target)
#     4.3  zmienne sÄ… skategoryzyowane, ale posiadajÄ… wewnÄ™trznÄ… rangÄ™    
#     4.4  Nie    
# 5.  Informacje, czy osoba osiÄ…gnie prÃ³g dochodu    
#     5.1  Zmianna logiczna o charakterze binarnym    
#     5.2  Tak, najwiÄ™ksza korelacja jest dla Martial Status: Married-civ-spous i Edukacion, najmnijesza: Never-married     
# 6.  Workclass, Occupation, Country   
# 7.  Education-Num, Sex

# **PodpowiedÅº**: sprÃ³buj uÅ¼yÄ‡ `train.info()`, `train.describe()` i `train.corr()`, Å¼eby znaleÅºÄ‡ odpowiedzi. RÃ³wnieÅ¼ funkcje jak `.unique()` lub `.nunique()` mogÄ… byÄ‡ przydatne (pierwsza zwraca wszystkie unikalne wartoÅ›ci dla danej cechy, druga iloÅ›Ä‡ unikalnych cech), np. `train.Age.nunique()` zwrÃ³ci 73, to oznacza, Å¼e mamy 73 rÃ³Å¼ne wartoÅ›ci dla cechy wiek.
# 
# DoÅ›Ä‡ czÄ™sto patrzÄ…c na dane, funkcja `.value_counts()` jest takÅ¼e przydatna, bo zlicza, ile razy pojawiÅ‚a siÄ™ konkretna wartoÅ›Ä‡ (np. ile mamy ludzi, ktÃ³rzy sÄ… w wieku 36 lat itd), moÅ¼na sprÃ³bowaÄ‡ w ten sposÃ³b `train.Age.value_counts()`.
# 
# *Nawiasem mÃ³wiÄ…c*: `pd.describe()` pomaga zrozumieÄ‡ tak zwane [5 liczb statystycznych](https://bit.ly/2O2caeV).

# In[3]:


#info


# In[4]:


#describe


# In[5]:


#corr


# In[ ]:


*Nawiasem mÃ³wiÄ…c*: `pd.corr()` wyglÄ…da ciekawiej, jeÅ›li to zwizualizujemy. Na szczÄ™Å›cie da siÄ™ to zrobiÄ‡ bardzo prosto.


# In[7]:


plt.rcParams['figure.figsize']=(20,10)
sns.heatmap(train.corr(), vmax=1., vmin=-1., annot=True, linewidths=.8, cmap="YlGnBu");


# MÃ³wiÄ…c o korelacji, przede wszystkim chodzi o korelacjÄ™ liniowÄ… (domyÅ›lnie `pd.corr()` uÅ¼ywa [Pearsona](https://bit.ly/3lZa1gq)). 
# 
# Po drugie, chodzi o korelacjÄ™ pomiÄ™dzy zmiennymi ciÄ…gÅ‚ymi (numerycznymi).
# 
# Co zrobiÄ‡ ze zmiennymi kategorialnymi (albo jeszcze ciekawszÄ… kombinacjÄ…: zmienna kategorialna i zmienna ciÄ…gÅ‚a)? To jest wiÄ™ksza "rozkmina", ktÃ³ra wykracza poza ten kurs. Natomiast na tej [stronie](https://bit.ly/2PxcORX) moÅ¼na zobaczyÄ‡ rÃ³Å¼ne kombinacje i inspiracje, jak sobie z tym radziÄ‡.

# In[8]:


#unique/nunique/value_counts


# ## Popatrz na dane
# Pierwsze 5 czy 10 wierszy
# 
# *Inaczej mÃ³wiÄ…c*: funkcja `.head()` domyÅ›lnie pokazuje tylko pierwsze **5 wierszy**, ale moÅ¼esz to zmieniÄ‡ przekazujÄ…c parametr, np. pokazaÄ‡ pierwsze **20 wierszy** `.head(20)`. 
# 
# MoÅ¼esz rÃ³wnieÅ¼ sprÃ³bowaÄ‡ uÅ¼yÄ‡ funkcji `.sample(10)`. Ta funkcja bÄ™dzie losowaÄ‡ `n` wierszy, gdzie `n` podajesz jako argument. JeÅ›li zbiÃ³r danych jest relatywnie maÅ‚y (mniej niÅ¼ milion), to moÅ¼e byÄ‡ to lepszÄ… opcjÄ…, bo po uruchomieniu za kaÅ¼dym razem moÅ¼esz zobaczyÄ‡ coÅ› wiÄ™cej niÅ¼ tylko pierwsze 10 wierszy. 
# 
# W ten sposÃ³b moÅ¼esz "wyÅ‚apaÄ‡" ciekawsze przypadki mniejszym kosztem (sprÃ³buj kilka razy uruchomiÄ‡ tÄ™ linijkÄ™).

# In[9]:


train.sample(10)


# ### Oficjalny opis danych
# 
# - **Age** â€“ Wiek osoby.
# - **Workclass** â€“ Rodzaj pracodawcy, jaki ma dana osoba. NiezaleÅ¼nie od tego, czy sÄ… to instytucje rzÄ…dowe, wojskowe, prywatne i inne.
# - **fnlwgt** â€“ Pewna liczba (maÅ‚o istotne jaka). BÄ™dziemy ignorowaÄ‡ tÄ™ zmiennÄ….
# - **Education** â€“ NajwyÅ¼szy poziom wyksztaÅ‚cenia osiÄ…gniÄ™ty dla tej osoby.
# - **Education-Num** â€“ NajwyÅ¼szy poziom wyksztaÅ‚cenia w formie liczbowej.
# - **Marital** â€“ Stan cywilny osoby.
# - **Occupation** â€“ Wykonywany zawÃ³d.
# - **Relationship** â€“ TrochÄ™ trudniejsze do wytÅ‚umaczenia. Zawiera wartoÅ›ci zwiÄ…zane z rodzinÄ…, takie jak mÄ…Å¼, ojciec itd.
# - **Race** â€“ opisy poszczegÃ³lnych ras czÅ‚owieka. 
# - **Sex** â€“ PÅ‚eÄ‡.
# - **Capital Gain** â€“ Zyski kapitaÅ‚owe rejestrowane.
# - **Capital Loss** â€“ Straty kapitaÅ‚owe rejestrowane.
# - **Hours per week** â€“ Liczba godzin przepracowanych w tygodniu.
# - **Country** â€“ Kraj pochodzenia danej osoby.
# - **Target** â€“ Zmienna logiczna (mniejsza, rÃ³wna siÄ™ lub wiÄ™ksza). NiezaleÅ¼nie od tego, czy dana osoba zarabia wiÄ™cej niÅ¼ 50 000$ rocznie.
# 
# 
# WiÄ™cej o danych moÅ¼na przeczytaÄ‡ [tutaj](https://bit.ly/39lBFPy).
# 
# SpÄ™dÅº proszÄ™ chwilÄ™ czasu zastanawiajÄ…c siÄ™ nad danymi. PomyÅ›l (na razie teoretycznie), ktÃ³ra cecha moÅ¼e mieÄ‡ wiÄ™kszy lub mniejszy wpÅ‚yw na jakoÅ›Ä‡ modelu. BÄ™dzie dobrze, jeÅ›li zanotujesz swoje pomysÅ‚y, a pÃ³Åºniej to bÄ™dziemy weryfikowaÄ‡. 
# 
# 
# ## QGrid
# CiekawostkÄ… jest to, Å¼e jest narzÄ™dzie [qgrid](https://bit.ly/31rXj0j) i funkcja `.show_grid()`, ktÃ³re umoÅ¼liwia analizowanie danych bez znajomoÅ›ci `pandas`. To moÅ¼e byÄ‡ krok przejÅ›ciowy :). Zobacz, jak to wyglÄ…da.

# In[10]:


qgrid.show_grid(train, show_toolbar=True)


# Teraz przeÅ‚Ä…czamy na siÄ™ kodowanie, bo jednak warto to poznaÄ‡, bo daje wiÄ™ksze moÅ¼liwoÅ›ci.
# 
# 
# ## Braki w danych
# Jest kilka cech w ktÃ³rych sÄ… braki. Kod poniÅ¼ej wypisze, ktÃ³re to cechy i jak duÅ¼o jest brakÃ³w?

# In[11]:


def check_missing():
    for column in train.columns:
        missing = column, train[column].isnull().sum()
        if missing[1] == 0: continue

        print(missing)
        
check_missing()


# ### Powstaje pytanie, co z tym robiÄ‡? 
# Jest wiele rÃ³Å¼nych technik, ale na poczÄ…tek moÅ¼emy zastosowaÄ‡ najprostszÄ…: "olaÄ‡ to" :). Tylko Å¼ycie jest trochÄ™ bardziej skomplikowane i pÃ³Åºniej "wysypie siÄ™" kod w innym miejscu, bo oczekuje tego, Å¼e bÄ™dÄ… dane. Dlatego zrÃ³bmy coÅ› bardzo prostego, a pÃ³Åºniej (jak zbudujemy caÅ‚y workflow do testowania w praktyce), bÄ™dziemy patrzeÄ‡, co jest lepsze.
# 
# Na poczÄ…tek zamiast wszystkich brakujÄ…cych wartoÅ›ci wstawimy: -1. 
# 
# ### Dlaczego -1? 
# CoÅ› trzeba wstawiÄ‡ :), ale liczba ujemna dlatego, Å¼eby nie powtarzaÄ‡ siÄ™. Na przykÅ‚ad doÅ›Ä‡ czÄ™stym bÅ‚Ä™dem jest wstawianie 0. Problem pojawia siÄ™ wtedy, gdy jest rÃ³Å¼nica pomiÄ™dzy wartoÅ›ciÄ… 0, ktÃ³ra byÅ‚a od samego poczÄ…tku i wartoÅ›ciÄ… 0, ktÃ³ra pojawiÅ‚a siÄ™ na skutek zmiany brakujÄ…cej wartoÅ›ci. Innymi sÅ‚owy, niechcÄ…cy bÄ™dziemy wtedy "oszukiwaÄ‡" nasz model.
# 
# WywoÅ‚ajmy funkcjÄ™ z pandas `.fillna()`, ktÃ³ra wypeÅ‚nia wszystkie brakujÄ…ce wartoÅ›ci liczbÄ… podanÄ… przez nas.

# In[12]:


train = train.fillna(-1)


# SprawdÅºmy na wszelki wypadek, czy to zadziaÅ‚a dobrze...
# 
# *Nawiasem mÃ³wiÄ…c*, wypracuj nawyk weryfikowania wszystkiego co najmniej raz (tym bardziej tych oczywistych rzeczy), co jakiÅ› czas wynik bÄ™dzie zaskakiwaÄ‡.

# In[13]:


check_missing()


# WyglÄ…da, Å¼e jest OK - mam wszystkie wartoÅ›ci :). Na wszelki wypadek moÅ¼na jeszcze zobaczyÄ‡ `train.info()` (ale juÅ¼ bez przesady).

# ## Metryka sukcesu
# 
# 
# Trzeba wybraÄ‡, w jaki sposÃ³b mierzyÄ‡ jakoÅ›Ä‡ modelu. MÃ³wimy o klasyfikacji (binarnej), wiÄ™c zwykle pierwszÄ… rzeczÄ…, ktÃ³ra przychodzi do gÅ‚owy to dokÅ‚adnoÅ›Ä‡ (ang. *accuracy*). Nie zawsze ta metryka jest dobra (zwÅ‚aszcza, jeÅ›li mÃ³wimy o skrzywionych [niezbilansowanych] zbiorach danych). 
# 
# Jest tak zwana [tablica pomyÅ‚ek](https://bit.ly/3w751er) ([confusion matrix](https://bit.ly/3sJrHz6)). Na razie tylko wspominam o istnieniu tej tablicy, ale jeszcze do niej wrÃ³cimy :).
# 
# DokÅ‚adnoÅ›Ä‡ (ang. *accuracy*) liczy siÄ™ bardzo prosto:
# $$ \frac{wszystkie\ poprawne\ odpowiedzi}{wszystkie\ odpowiedzi}$$
# 
# Pobawmy siÄ™ trochÄ™, Å¼eby wyczuÄ‡ to lepiej. W kaÅ¼dym wierszu sÄ… dwie listy, z lewej strony prawidÅ‚owa odpowiedÅº, a z prawej strony jest predykcja (czyli "odpowiedÅº" modelu).

# In[14]:


values = [
      #odpowiedÅº          #predykcja
    ([1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0]), #dokÅ‚adnoÅ›Ä‡ 100% 
    ([1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0]), #dokÅ‚adnoÅ›Ä‡ 50% 
    ([1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1]), #dokÅ‚adnoÅ›Ä‡ 50% 
    ([1, 1, 1, 0, 0, 0], [1, 1, 1, 0, 1, 1]), #dokÅ‚adnoÅ›Ä‡ ~67% 
]

for true_values, pred_values in values:
    score = accuracy_score(true_values, pred_values)
    print("score: [{0}], true: {1}, pred: {2}".format(score, true_values, pred_values))


# ## Pytania 1.5.2
# 
# 1. Jaka jest minimalna i maksymalna wartoÅ›Ä‡ dokÅ‚adnoÅ›ci?
# 2. Jaka jest minimalna i maksymalna wartoÅ›Ä‡ dokÅ‚adnoÅ›ci w przykÅ‚adzie, ktÃ³ry podaÅ‚em (zakÅ‚adajÄ…c, Å¼e odpowiedzi sÄ… staÅ‚e)?
# 3. Kiedy dokÅ‚adnoÅ›Ä‡ jest rÃ³wna zero?
# 4. Jak zachowuje siÄ™ dokÅ‚adnoÅ›Ä‡ na niezbilansowanych zbiorach danych (np. prawie wszystko 0 i tylko kilka jedynek, standardowy przykÅ‚ad dla zadaÅ„ typu `fraud detection`)?

# ## Odpowiedzi/wnioski 1.5.2
# 1. od 0 do 1    
# 2. od 0,5 do 1  
# 3. Kiedy wszystkie odpowiedzi sÄ… bÅ‚Ä™dne    
# 4. FaÅ‚szywe wysokie wyniki, ze wzglÄ™du na przypisanie wszystkim rekordom tego samego wyniku    

# ## Basic Model
# 
# Zbuduj najprostszy model...
# 
# **PodpowiedÅº:** uÅ¼yj [DummyClassifier](https://bit.ly/3w7O17T)
# 
# Przed trenowaniem modelu trzeba przygotowaÄ‡ dane. ZrÃ³bmy to najszybciej, jak tylko siÄ™ da. Dla `DummyClassifier` wszystko jedno jaka jest cecha (waÅ¼ne, Å¼eby tylko rozmiar macierzy siÄ™ zgadzaÅ‚). Jest to istotne, poniewaÅ¼ model patrzy tylko na zmiennÄ… docelowÄ… i w zaleÅ¼noÅ›ci od strategii zwraca zawsze wartoÅ›Ä‡ Å›redniÄ… lub medianÄ™ (dla przykÅ‚adu zaÅ‚Ã³Å¼my, Å¼e strategiÄ… bÄ™dzie wartoÅ›Ä‡ Å›rednia, wiÄ™c bierzemy wszystkie odpowiedzi, znajdujemy wartoÅ›Ä‡ Å›redniÄ… i model zawsze bÄ™dzie "odpowiadaÅ‚", Å¼e wynik to wartoÅ›Ä‡ Å›rednia).
# 
# **ZapamiÄ™taj**: wszystkie modele wewnÄ…trz oczekujÄ… na liczby (a jeÅ›li jest inaczej, to oznacza, Å¼e model jest na tyle sprytny, Å¼e jest w stanie sam zrobiÄ‡ to przeksztaÅ‚cenie). W naszym przypadku `Target` jest w tej chwili tekstowy, wiÄ™c trzeba to przerzuciÄ‡ do klasy binarnej. 
# 
# Naszym zadaniem byÅ‚o przewidzieÄ‡, czy czÅ‚owiek bÄ™dzie zarabiaÅ‚ wiÄ™cej niÅ¼ 50K, odpowiedÅº "tak" jest dla tego warunku: `train['Target'] != '<=50K'`.
# 
# *Nawiasem mÃ³wiÄ…c*, `X` to jest macierz, ale `y` to wektor i to dlatego `X` jest pisany z duÅ¼ej litery (`y` z maÅ‚ej), taka jest konwencja.

# In[15]:


train['target_cat'] = (train['Target'] != '<=50K').astype('int8')

X = train[ ['Age'] ].values ##to macierz, dlatego sÄ… podwÃ³jne nawiasy, natomiast teraz to jest macierz z jednÄ… kolumnÄ…
y = train[ 'target_cat' ].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

print("Train:", X_train.shape, y_train.shape)
print("Test:", X_test.shape, y_test.shape)


# Zamiast `YOUR CODE HERE` trzeba podaÄ‡ model,  ktÃ³rego bÄ™dziesz uÅ¼ywaÄ‡ (np. dummy czy liniowy).

# In[17]:


#model = ...YOUR CODE HERE

model.fit(X_train, y_train)    #podajemy X_train, y_train
y_pred = model.predict(X_test) #sprawdzamy na X_test

print(accuracy_score(y_test, y_pred))


# DobrÄ… praktykÄ… bÄ™dzie poprzedniÄ… komÃ³rkÄ™ "wrzuciÄ‡" do jednej funkcji i jako argument bÄ™dzie podawany model.

# In[18]:


def train_and_predict(model, X, y, test_size=0.33):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2018)
    
    model.fit(X_train, y_train)    
    y_pred = model.predict(X_test) 

    return accuracy_score(y_test, y_pred)


# SprawdÅº, czy wynik jest podobny.

# In[19]:


train_and_predict(DummyClassifier(strategy = 'stratified'), X, y, test_size=0.33)


# ## Model Basic+
# 
# ZrÃ³bmy trochÄ™ bardziej zÅ‚oÅ¼ony model. Zwykle bierze siÄ™ liniowy. Teraz juÅ¼ model "patrzy" na cechy, wiÄ™c na poczÄ…tek moÅ¼na uÅ¼ywaÄ‡ nadal jednej ("`Age`"), ale w kolejnych iteracjach dodawaÄ‡ kolejne cechy (ale pamiÄ™taj, o tym, Å¼e jeÅ›li cecha nie jest liczbowa, to trzeba zrobiÄ‡ transformacjÄ™).
# 
# **PodpowiedÅº**: uÅ¼yj [LogisticRegression](https://bit.ly/3rCEu5q)
# 
# *Nawiasem mÃ³wiÄ…c*, problem ktÃ³ry rozwiÄ…zujemy jest klasyfikacjÄ… (binarnÄ…). Czemu zatem model zawiera nazwÄ™ "regression"? Jest to mylÄ…ce, ale nazwa juÅ¼ siÄ™ przyczepiÅ‚a ... W pewnym sensie da siÄ™ wytÅ‚umaczyÄ‡, dlaczego akurat nazwa jest taka, ale fakt jest faktem, jest ona mylÄ…ca :).

# In[20]:


#train_and_predict(..., X, y) #zamiast kropek, trzeba wstawiÄ‡ model np. 


# ## Feature Engineering
# Zacznijmy generowaÄ‡ nowe cechy.
# 
# Zmienne kategorialne bÄ™dÄ… zamienione na liczby. MoÅ¼emy to zrobiÄ‡ co najmniej na dwa sposoby.
# 
# ### 1. Label Encoding
# Åatwiejszy sposÃ³b polega na tym, aby do kaÅ¼dej unikalnej wartoÅ›ci przypisaÄ‡ ID
# np. `jabÅ‚ko => 1, gruszka => 2, pomaraÅ„cza => 3` itd.
# 
# ##### Zalety
# Jest szybkie i proste. Tanie z punktu widzenia zasobÃ³w (nadal mamy jednÄ… kolumnÄ™, zwykle potrzebujemy mniej miejsca niÅ¼ przed tym).
# 
# ##### Wady
# `jabÅ‚ko` to nie jest to samo co `1`, chociaÅ¼by dlatego Å¼e `1` jest mniejsze niÅ¼ `2`, czy jabÅ‚ko jest mniejsze niÅ¼ gruszka? Innymi sÅ‚owy, niechcÄ…cy prÃ³bujemy "oszukaÄ‡" nasz model skrzywiajÄ…c rzeczywistoÅ›Ä‡.
# 
# ### 2. [One Hot](https://bit.ly/39j8GMv) Encoding
# Dlatego pojawia siÄ™ od razu co najmniej jeszcze jedna alternatywa, ktÃ³ra prÃ³buje rozwiÄ…zaÄ‡ problem poprzedniego sposobu (ale pamiÄ™tasz, Å¼e jak to zwykle bywa "coÅ› za coÅ›").
# 
# DziaÅ‚a to tak, Å¼e dla kaÅ¼dej unikalnej wartoÅ›ci pojawia siÄ™ pytanie: czy to jest X? Na przykÅ‚ad, "czy to jest jabÅ‚ko?" lub "czy to jest gruszka?".
# 
# 
# ##### Zalety
# RozwiÄ…zaliÅ›my poprzedni problem (kiedy "jabÅ‚ko" mogÅ‚o byÄ‡ "mniejsze", niÅ¼ gruszka).
# 
# ##### Wady
# PotrzebujÄ™ znacznie wiÄ™cej zasobÃ³w. Dla kaÅ¼dej unikalnej wartoÅ›ci pojawia siÄ™ nowa kolumna (szczegÃ³lnie to zaczyna boleÄ‡, gdy unikalnych wartoÅ›ci sÄ… dziesiÄ…tki, a tym bardziej setki).
# RÃ³wnieÅ¼ przy wiÄ™kszej iloÅ›ci cech model jest bardziej podatny na przeuczenie siÄ™.
# 
# **ZapamiÄ™taj** Dla obu przypadkÃ³w trzeba z gÃ³ry znaÄ‡ wszystkie unikalne wartoÅ›ci, bo inaczej pojawia siÄ™ problem z interpretacjÄ… (i tak teÅ¼ siÄ™ dzieje, sÄ… na to dodatkowe techniki, do tego jeszcze wrÃ³cimy).
# 
# *Nawiasem mÃ³wiÄ…c*, nie ma jednej dobrej porady, co jest lepsze (bo inaczej istniaÅ‚aby tylko jedna [dobra] technika). Jak to zwykle bywa w Å¼yciu, trzeba prÃ³bowaÄ‡ zaczynajÄ…c od najprostszych rzeczy, jeÅ›li jest to wystarczajÄ…co dobre, to cieszymy siÄ™, jeÅ›li nie - prÃ³bujemy innych metod bardziej zÅ‚oÅ¼onych.
# 
# 
# ZrÃ³bmy eksperyment, Å¼eby lepiej zrozumieÄ‡ w praktyce, jak to dziaÅ‚a... Dla przykÅ‚adu bierzemy kolumnÄ™ (cechÄ™) `Race`, ktÃ³ra ma 5 unikalnych wartoÅ›ci. Po zastosowaniu `one-hot encoding` pojawi siÄ™ piÄ™Ä‡ nowych kolumn, gdzie wartoÅ›ci w kolumnach bÄ™dÄ… 0 lub 1. Na przykÅ‚ad, `isWhite`: 0 lub 1 (tak lub nie) itd.

# In[21]:


train['Race'].unique()


# W pandas jest funkcja `.get_dummies()`, ktÃ³ra umoÅ¼liwia w prosty sposÃ³b uruchomiÄ‡ `one-hot-encoding` transformacjÄ™.

# In[22]:


pd.get_dummies( train['Race'] ).head()


# ## Label Encoding
# Jednym z najprostszych sposobÃ³w jest uÅ¼ycie funkcji pandas `pd.factorize()` i pobranie pierwszego elementu (drugim jest label, a nam jest potrzebne tylko ID).
# 
# RÃ³wnieÅ¼ moÅ¼na uÅ¼ywaÄ‡ funkcji z `sklearn`: [LabelEncoder](https://bit.ly/3ruF591). Tylko wtedy bÄ™dzie wiÄ™cej linii kodu :).

# In[23]:


pd.factorize( ['a', 'b', 'c', 'a', 'a', 'c'] )


# Funkcja `pd.factorize()` przyjmuje listÄ™ (zwykle chodzi o ciÄ…g znakÃ³w, oczywiÅ›cie rÃ³wnieÅ¼ moÅ¼e byÄ‡ to ciÄ…g liczb, tylko po co konwertowaÄ‡ liczby w liczby?). Przypisuje unikalny ID dla danej wartoÅ›ci i zwracane sÄ… dwie listy. Pierwsza lista z ID'kami, a druga to lista unikalnych wartoÅ›ci.
# 
# W przypadku powyÅ¼ej, unikalne ID wyglÄ…dajÄ… tak:
# - a => 0
# - b => 1
# - c => 2
# 
# #### input
# `['a', 'b', 'c', 'a', 'a', 'c']`
# 
# #### output
# `[0, 1, 2, 0, 0, 2]`
# 
# W jaki sposÃ³b `pd.factorize()` to robi? 
# 1. Na poczÄ…tku funkcja tworzy pustÄ… listÄ™ `[]`.
# 2. NastÄ™pnie bierze naszÄ… kolumnÄ™ i po kolei sprawdza kaÅ¼dy element z tej kolumny.
# 3. Dla kaÅ¼dego elementu sprawdza, czy ten element widzi po raz pierwszy, czy po raz kolejny.
# 4. JeÅ›li po raz pierwszy, to dodaje ten element do tej listy, ktÃ³rÄ… utworzyÅ‚ na poczÄ…tku i indeks tej wartoÅ›ci w tej liÅ›cie bÄ™dzie liczbÄ…, ktÃ³rÄ… bÄ™dzie od tej pory przypisywaÅ‚ do tej wartoÅ›ci. 
# 5. JeÅ›li natomiast trafia na element, ktÃ³ry juÅ¼ byÅ‚ wczeÅ›niej, to po prostu przypisuje mu jego indeks z tej listy.
# 
# Wynik na koÅ„cu bÄ™dzie skÅ‚adaÅ‚ siÄ™ z 2-Ã³ch elementÃ³w.
# Pod indeksem 0 bÄ™dzie nasza kolumna zamieniona na liczby. Pod indeksem 1 bÄ™dzie nasza lista unikalnych wartoÅ›ci z kolumny.
# JeÅ›li wiÄ™c zrobisz np. `pd.factorize(..)[1][0]` to zobaczysz, jaka wartoÅ›Ä‡ zostaÅ‚a zamieniona na 0 :).
# 
# Wracamy do naszego przykÅ‚adu.

# In[24]:


pd.factorize( train['Workclass'] )[0]


# SprÃ³bujmy `LabelEncoder`, ale od razu powiem, Å¼e zaraz wyskoczy bÅ‚Ä…d. Nie przejmuj siÄ™ tym. Za chwilÄ™ zrozumiemy, co siÄ™ staÅ‚o.
# 
# *Nawiasem mÃ³wiÄ…c*, jak pojawia siÄ™ jakiÅ› bÅ‚Ä…d, to staraj siÄ™ przewijaÄ‡ na sam koniec, tam zwykle sÄ… najciekawsze informacje (np. w ostatniej linijce).

# In[26]:


le = LabelEncoder()
le.fit(train['Workclass'])
le.transform(train['Workclass'])


# No i wÅ‚aÅ›nie `TypeError: Encoders require their input to be uniformly strings or numbers` narzeka na to, Å¼e mamy `str` i `int`.  To popatrzmy, jakie mamy unikalne wartoÅ›ci dla `Workclass`.

# In[27]:


train.Workclass.unique()


# PamiÄ™taj, Å¼e `Workclass` miaÅ‚ brakujÄ…ce wartoÅ›ci i na szybko wrzuciliÅ›my tam -1? No wÅ‚aÅ›nie w tym przypadku oczekuje od nas tylko `string`. Szybko to naprawiamy i...

# In[28]:


train.Workclass = train.Workclass.map(lambda x: str(x))

le = LabelEncoder()
le.fit(train['Workclass'])
le.transform(train['Workclass'])


# Jak widaÄ‡, dziaÅ‚a :). Super! Tylko maÅ‚o tego, Å¼e trzeba napisaÄ‡ wiÄ™cej kodu, to jeszcze musieliÅ›my trzymaÄ‡ wszystkie dane jednego typu (`string`), natomiast `pd.factorize()` poradziÅ‚ sobie z tym bardzo dobrze. RÃ³wnieÅ¼ `.factorize()` jest szybszy, to jest bardzo zauwaÅ¼alne na wiÄ™kszych zbiorach danych (dlatego lubiÄ™ `.factorize()`).
# 
# Ale...
# 
# `LabelEncoder()` ma teÅ¼ swoje zalety, jednÄ… z najwiÄ™kszych jest Å‚Ä…czenie rÃ³Å¼nych elementÃ³w w tak zwany [pipeline](https://bit.ly/3cxZxBQ). Na razie nie potrzebujemy tego, wiÄ™c tylko warto wiedzieÄ‡, Å¼e coÅ› takiego jest.
# 
# ### Czy pamiÄ™tasz, co robiliÅ›my? 
# No wÅ‚aÅ›nie chcemy przygotowaÄ‡ cechy kategorialne, Å¼eby mÃ³c wytrenowaÄ‡ model. Model oczekuje liczb. To juÅ¼ wiemy, jak konwertowaÄ‡ zmienne kategorialne, musimy tylko je znaleÅºÄ‡. Podpowiem Ci jeden `trick`, ktÃ³ry zwykle dziaÅ‚a dobrze w wiÄ™kszoÅ›ci przypadkÃ³w.

# In[29]:


train.info()


# DoÅ›Ä‡ czÄ™sto to, co pokazuje siÄ™ w `.info()` jako "object" jest zmiennÄ… kategorialnÄ… (choÄ‡ czasem to moÅ¼e byÄ‡ data lub jakaÅ› pokrÄ™cona liczba). StÄ…d wniosek jest taki, Å¼e Å¼eby znaleÅºÄ‡ wszystkie zmienne kategorialne trzeba odfiltrowaÄ‡ po typie (zostawiÄ‡ tylko "object").

# In[30]:


train.select_dtypes(include=[np.object]).columns


# In[31]:


cat_feats = train.select_dtypes(include=[np.object]).columns

for cat_feat in cat_feats:
    train['{0}_cat'.format(cat_feat)] = pd.factorize( train[cat_feat] )[0]


# **ZapamiÄ™taj**, jak transformujesz zmiennÄ…, to twÃ³rz lepiej nowÄ… kolumnÄ™, nie nadpisuj (chyba Å¼e juÅ¼ byÅ‚o sprawdzone kilka razy), bo inaczej moÅ¼esz straciÄ‡ oryginalnÄ… wartoÅ›Ä‡. 
# 
# *Inaczej mÃ³wiÄ…c*, dodajÄ™ prefix `_cat`, i to nie chodzi o to, Å¼e lubiÄ™ koty (chociaÅ¼ to teÅ¼ prawdağŸ˜º), to bardziej skrÃ³t od zmiennej kategorialnej (`category`)... aha i to moja wÅ‚asna konwencja, ktÃ³rej siÄ™ trzymam. Natomiast moÅ¼esz wymyÅ›liÄ‡ swojÄ…, jeÅ›li masz powody :).
# 
# To wrÃ³Ä‡my do naszego liniowego modelu i dodajmy mu wiÄ™cej cech (kategorialne i iloÅ›ciowe).

# In[32]:


feats = train.select_dtypes(include=[np.int]).columns.values
feats


# **Target_cat** to jest zmienna docelowa, wiÄ™c trzeba to usunÄ…Ä‡ z cech, a poza tym jest ok.

# In[33]:


feats = feats[:-1] #-1 oznacza ostatni element, ktÃ³ry wycinamy
feats


# In[34]:


X = train[ feats ].values
y = train['Target_cat'].values


# In[35]:


train_and_predict(YOUR_CODE_HERE, X, y, test_size=0.33) ### podaj model (np. LogisticRegression())


# ## Zadania domowe
# 
# Kilka wskazÃ³wek jak lepiej badaÄ‡ dane. Dla zmiennych numerycznych uÅ¼ywaj `.hist()`.

# In[36]:


train['Education-Num'].hist();


# Dla innych zmiennych (np. kategorialnych) to nie przejdzie i wtedy moÅ¼na zrobiÄ‡ zadanie w ten sposÃ³b:

# In[37]:


train['Education'].value_counts().plot(kind='bar');


# lub moÅ¼na uÅ¼yÄ‡ `seaborn`:

# In[38]:


plt.figure(figsize=(15, 5))
sns.countplot(x='Education', data=train);
plt.xticks(rotation=90);


# ## Zadanie 1.5.3
# 
# SprÃ³buj zbadaÄ‡ kolejne cechy, to moÅ¼e byÄ‡ przydatne, Å¼eby zaczÄ…Ä‡ tworzyÄ‡ lepsze cechy.

# In[39]:


print(train.columns)
train.info()

plt.figure(figsize=(15, 5))
feats = train.select_dtypes(include=[np.object]).columns[:-1]
feats

for feat in feats:
    plt.figure(figsize=(15, 5))
    plt.title(feat)
    #train[feat].value_counts().plot(kind='bar')
    sns.countplot(x=feat, data=train);
    plt.xticks(rotation=90);
    plt.show();


# 
# 
# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ odpowiedÅº ğŸ‘ˆ </summary>
# <p>
# 
# ```python
# print(train.columns)
# 
# 
# plt.figure(figsize=(15, 5))
# sns.countplot(x='Workclass', data=train);
# plt.xticks(rotation=90);
# plt.show()
# 
# plt.figure(figsize=(15, 5))
# sns.countplot(x='Martial Status', data=train);
# plt.xticks(rotation=90);
# plt.show()
# 
# plt.figure(figsize=(15, 5))
# sns.countplot(x='Relationship', data=train);
# plt.xticks(rotation=90);
# plt.show()
# 
# ```
#  
# </p>
# </details>
# </p>
# </details> 

# Zobaczmy, jak wyglÄ…da rozkÅ‚ad danych. To pomoÅ¼e Ci zobaczyÄ‡ pewne zaleÅ¼noÅ›ci i zaczÄ…Ä‡ tworzyÄ‡ kolejne cechy (oparte na kombinacji istniejÄ…cych cech).
# 
# Zaczniemy od zbadania: pÅ‚ci oraz edukacji. Przypominam, Å¼e na osi y, jest prawdopodobieÅ„stwo, Å¼e ta osoba zarobi wiÄ™cej niÅ¼ 50k rocznie (1.0 oznacza 100%).

# In[40]:


plt.figure(figsize=(15, 5))
sns.barplot(x="Education", y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Zbadajmy teraz rasÄ™ i pÅ‚eÄ‡.

# In[41]:


plt.figure(figsize=(15, 5))
sns.barplot(x="Race", y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Zbadajmy kraj pochodzenia oraz pÅ‚eÄ‡.

# In[42]:


plt.figure(figsize=(15, 5))
sns.barplot(x="Country", y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Zbadajmy stan cywilny oraz pÅ‚eÄ‡.

# In[43]:


plt.figure(figsize=(15, 5))
sns.barplot(x='Martial Status', y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Zbadajmy zawÃ³d oraz pÅ‚eÄ‡.

# In[44]:


plt.figure(figsize=(15, 5))
sns.barplot(x='Occupation', y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Mam nadziejÄ™, Å¼e juÅ¼ widaÄ‡, ktÃ³re grupy wyrÃ³Å¼niajÄ… siÄ™ (a to brzmi jak cecha). Tu przy okazji zbadaliÅ›my temat niesprawiedliwoÅ›ci tego Å›wiata. Co jest waÅ¼ne, model nic nie wie na temat dyskryminacji, jedynie uczy siÄ™ z tego co jest "myÅ›lÄ…c", Å¼e to jest normalne. StÄ…d wÅ‚aÅ›nie pojawia siÄ™ bias, zobacz ten [filmik](https://www.youtube.com/watch?v=59bMh59JQDo). Musisz na to uwaÅ¼aÄ‡! Model staje siÄ™ tym, czym go karmisz :). Podobnie jak nasz mÃ³zg (to co tam wrzucamy, wpÅ‚ywa na to, kim siÄ™ stajemy pÃ³Åºniej).
# 
# 
# Zbadaj teraz jeszcze inne kombinacje np. zamiast pÅ‚ci sprawdÅº rasÄ™.

# In[45]:


plt.figure(figsize=(15, 5))
sns.barplot(x='Occupation', y="Target_cat", hue='Race', data=train)
plt.xticks(rotation=90);


# Zobacz jeszcze jednÄ… wskazÃ³wkÄ™ co do wizualizacji. MoÅ¼esz rozbiÄ‡ to na osobne wykresy.

# In[46]:


plt.figure(figsize=(20, 5))
g = sns.catplot(x="Occupation", y="Target_cat", col="Sex", data=train, kind="bar")

for ax in g.axes.flatten():
    plt.sca(ax)
    plt.xticks(rotation=90)


# JeÅ›li jest wiÄ™cej niÅ¼ 5, to moÅ¼na powiedzieÄ‡, ile ma byÄ‡ maksymalnie w jednym wierszu (uÅ¼ywajÄ…c `col_wrap`).

# In[47]:


plt.figure(figsize=(20, 5))
g = sns.catplot(x="Occupation", y="Target_cat", hue="Sex", col='Race', col_wrap=3, data=train, kind="bar")

for ax in g.axes.flatten():
    plt.sca(ax)
    plt.xticks(rotation=90)


# ZwrÃ³Ä‡ uwagÄ™ na czarne pionowe kreski (w sÅ‚upkach). Traktuj to jak rozrzut danych, jeÅ›li ta kreska jest zbyt zmienna (np. zobacz ostatni wykres), to ciÄ™Å¼ko cokolwiek wnioskowaÄ‡, bo jest zbyt duÅ¼a zmiennoÅ›Ä‡.
# 
# JuÅ¼ sporo wiesz, Å¼eby dalej poruszaÄ‡ siÄ™ samodzielnie. SprÃ³buj zrobiÄ‡ nastÄ™pujÄ…ce rzeczy.

# ## Zadanie 1.5.4
# Dodaj kolejne cechy (*features*) bazujÄ…c na tych, ktÃ³re juÅ¼ sÄ… (np. na podstawie dwÃ³ch cech `Relationship` i `Race`, moÅ¼na wykombinowaÄ‡ kilka rÃ³Å¼nych cech, np. `White` + `Husband` lub `Black` + `Husband`).
# 
# Moje oczekiwania sÄ… takie, Å¼e sprÃ³bujesz stworzyÄ‡ kilka czy kilkanaÅ›cie nowych cech. Od razu powiem, przygotuj siÄ™, Å¼e czÄ™sto nowa cecha moÅ¼e byÄ‡ maÅ‚o wartoÅ›ciowa. Natomiast wartoÅ›ciÄ… bÄ™dzie, jeÅ›li nauczysz siÄ™ szybko iterowaÄ‡ hipotezy (czyli odkrywaÄ‡ nowe cechy, ktÃ³re wnoszÄ… wartoÅ›Ä‡ poprzez szybkie eksperymenty).
# 
# Ciekawostka: byÄ‡ moÅ¼e warto dodaÄ‡ zmiennÄ… waga? Zobacz ten artykuÅ‚: [People who are overweight get paid less, according to a new LinkedIn study](https://bit.ly/39njuch)

# Daj znaÄ‡ na Slacku, czy udaÅ‚o Ci siÄ™ sprawdziÄ‡ tÄ™ cechÄ™ :) 

# In[48]:


train['Black_Husband'] = train['Race'].map(lambda y: int(y == 'Black')) * train['Relationship'].map(lambda z: int(z == 'Husband'))

plt.figure(figsize=(10,4))
sns.barplot(x='Black_Husband', y="Target", data=train)
plt.xticks(rotation=90)


# In[1]:


train["relationship_race"] = train.apply(lambda x: "{}-{}".format(x["Relationship"], x["Race"]), axis=1)
train["relationship_race_cat"] = train["relationship_race"].factorize()[0]


# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ podpowiedÅº ğŸ‘ˆ </summary>
# <p>
# UÅ¼ywajÄ…c Relationship i Race utwÃ³rz nowÄ… kolumnÄ™ (atrybut).
# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ odpowiedÅº ğŸ‘ˆ </summary>
# <p>
# 
# ```python
#     
# train["relationship_race"] = train.apply(lambda x: "{}-{}".format(x["Relationship"], x["Race"]), axis=1)
# train["relationship_race_cat"] = train["relationship_race"].factorize()[0]
# 
# #generate more ;)
# 
# ```
#  
# </p>
# </details>
# </p>
# </details> 

# ## Zadanie 1.5.5
# Zastosuj bardziej zÅ‚oÅ¼ony model, np. [DecisionTreeClassifier](https://bit.ly/39qD4Vk). Dlaczego akurat ten? Bo jest relatywnie prosty, ale znacznie skuteczniejszy niÅ¼ `Dummy` model. WiÄ™cej o drzewach decyzyjnych bÄ™dzie w nastÄ™pnych moduÅ‚ach. Dlatego na razie moÅ¼esz to potraktowaÄ‡ jako czarne pudÅ‚o.

# Koniecznie napisz na Slacku, jak juÅ¼ zrobisz to zadanie i podziel siÄ™ wynikiem. ZaÅ‚Ä…cz screen! 

# In[4]:


from sklearn.tree import DecisionTreeClassifier
def train_and_predict_model(X_train, X_test, y_train, y_test, model, success_metric=accurancy_score):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return success_metric(y_test, y_pred)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random.state=2018)
lr = train_and_predict_model(X_train, X_test, y_train, y_test, LogisticRegression())
print('LogisticRegression: ', lr)
print('')
dtc = train_and_predict_model(X_train, X_test, y_train, y_test, DecisionTreeClassifier())
print('DecisionTreeClassifier: ', dtc)
train.columns


# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ podpowiedÅº  ğŸ‘ˆ </summary>
# <p>
# UÅ¼yj DecisionTreeClassifier, ale zwrÃ³Ä‡ uwagÄ™ na max_depth
# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ odpowiedÅº  ğŸ‘ˆ </summary>
# <p>
# 
# ```python
# train_and_predict(DecisionTreeClassifier(max_depth=5), X, y, test_size=0.33)
# ```
# 
# </p>
# </details>
# </p>
# </details> 

# ### ğŸ¤ğŸ—£ï¸ WspÃ³Å‚praca ğŸ’ª i komunikacja ğŸ’¬
# 
# - ğŸ‘‰ [#pml_module1](https://practicalmlcourse.slack.com/archives/C045CNLNH89) - to jest miejsce, gdzie moÅ¼na szukaÄ‡ pomocy i dzieliÄ‡ siÄ™ doÅ›wiadczeniem - takÅ¼e pomagaÄ‡ innym ğŸ¥°. 
# 
# JeÅ›li masz pytanie, to staraj siÄ™ jak najdokÅ‚adniej je sprecyzowaÄ‡, najlepiej wrzuÄ‡ screen z twoim kodem i bÅ‚Ä™dem, ktÃ³ry siÄ™ pojawiÅ‚ âœ”ï¸
# 
# - ğŸ‘‰ [#pml_module1_done](https://practicalmlcourse.slack.com/archives/C045CP89KND) - to miejsce, gdzie moÅ¼esz dzieliÄ‡ siÄ™ swoimi przerobionymi zadaniami, wystarczy, Å¼e wrzucisz screen z #done i numerem lekcji np. *#1.2.1_done*, Å›miaÅ‚o dodaj komentarz, jeÅ›li czujesz takÄ… potrzebÄ™, a takÅ¼e rozmawiaj z innymi o ich rozwiÄ…zaniach ğŸ˜Š 
# 
# - ğŸ‘‰ [#pml_module1_ideas](https://practicalmlcourse.slack.com/archives/C044TFZLF1U)- tutaj moÅ¼esz dzieliÄ‡ siÄ™ swoimi pomysÅ‚ami

# ### Podziel siÄ™ ze Å›wiatem swoimi nowymi umiejÄ™tnoÅ›ciami ğŸ‘
# 
# Za TobÄ… tydzieÅ„ nauki i nowe umiejÄ™tnoÅ›ci / doÅ›wiadczenia. Pochwal siÄ™ tym na swoim profilu LinkedIn âœ”ï¸
# 
# Dlaczego warto to zrobiÄ‡? ğŸ¤” 
# 
# Przede wszystkim jest czym siÄ™ pochwaliÄ‡, bo wykonujesz wÅ‚aÅ›nie w swoim Å¼yciu fajny krok w kierunku lepszej pracy i moÅ¼liwoÅ›ci zawodowych, umiesz coÅ› nowego, a wiÄ™c Twoi potencjalni pracodawcy powinni o tym wiedzieÄ‡ ;) 
# 
# Dodaj *#dataworkshop #newskills #practicalmachinelearning #datascience*
# 
# Oznacz nas! BÄ™dzie nam miÅ‚o. Nasz profil znajdziesz [tutaj](https://bit.ly/2Py5eGK) ğŸ¥° 

# ## Przydatne linki:
# - [Managing Large Datasets with Python and HDF5](https://bit.ly/31t3HEE)
# - [Guide to Encoding Categorical Values in Python](https://bit.ly/2P6fHcJ)
# - [Who can earn more than 50K per year?](https://bit.ly/2O2LgDJ)
# 
# - [Qgrid is a Jupyter notebook widget](https://bit.ly/3d9BQPe)

# In[ ]:




