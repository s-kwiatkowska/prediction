#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

#models
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.preprocessing import LabelEncoder

import qgrid

import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')


# ## Wczytujemy dane
# Dane sÄ… przechowywane w formacie [HDF5](https://bit.ly/3w6jbwk). To jest binarny format, ktÃ³ry jest doÅ›Ä‡ wygodny (zwykle trzymamy dane w tym formacie zamiast .csv). MiÄ™dzy innymi umoÅ¼liwia to zapisanie wiÄ™cej niÅ¼ jeden zbiÃ³r danych do jednego pliku.

# In[2]:


train = pd.read_hdf('../input/train.adult.h5')

# ## Zadanie 1.5.3
# 
# SprÃ³buj zbadaÄ‡ kolejne cechy, to moÅ¼e byÄ‡ przydatne, Å¼eby zaczÄ…Ä‡ tworzyÄ‡ lepsze cechy.

# In[39]:


print(train.columns)
train.info()

plt.figure(figsize=(15, 5))
feats = train.select_dtypes(include=[np.object]).columns[:-1]
feats

for feat in feats:
    plt.figure(figsize=(15, 5))
    plt.title(feat)
    #train[feat].value_counts().plot(kind='bar')
    sns.countplot(x=feat, data=train);
    plt.xticks(rotation=90);
    plt.show();


# 
# 
# <details>
#     <summary style="background: #e6eaeb; padding: 4px 0; text-align: center; font-size: 20px; font-weight: 900;"> ğŸ‘‰ Kliknij tutaj (1 klik), aby zobaczyÄ‡ odpowiedÅº ğŸ‘ˆ </summary>
# <p>
# 
# ```python
# print(train.columns)
# 
# 
# plt.figure(figsize=(15, 5))
# sns.countplot(x='Workclass', data=train);
# plt.xticks(rotation=90);
# plt.show()
# 
# plt.figure(figsize=(15, 5))
# sns.countplot(x='Martial Status', data=train);
# plt.xticks(rotation=90);
# plt.show()
# 
# plt.figure(figsize=(15, 5))
# sns.countplot(x='Relationship', data=train);
# plt.xticks(rotation=90);
# plt.show()
# 
# ```
#  
# </p>
# </details>
# </p>
# </details> 

# Zobaczmy, jak wyglÄ…da rozkÅ‚ad danych. To pomoÅ¼e Ci zobaczyÄ‡ pewne zaleÅ¼noÅ›ci i zaczÄ…Ä‡ tworzyÄ‡ kolejne cechy (oparte na kombinacji istniejÄ…cych cech).
# 
# Zaczniemy od zbadania: pÅ‚ci oraz edukacji. Przypominam, Å¼e na osi y, jest prawdopodobieÅ„stwo, Å¼e ta osoba zarobi wiÄ™cej niÅ¼ 50k rocznie (1.0 oznacza 100%).

# In[40]:


plt.figure(figsize=(15, 5))
sns.barplot(x="Education", y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Zbadajmy teraz rasÄ™ i pÅ‚eÄ‡.

# In[41]:


plt.figure(figsize=(15, 5))
sns.barplot(x="Race", y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Zbadajmy kraj pochodzenia oraz pÅ‚eÄ‡.

# In[42]:


plt.figure(figsize=(15, 5))
sns.barplot(x="Country", y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Zbadajmy stan cywilny oraz pÅ‚eÄ‡.

# In[43]:


plt.figure(figsize=(15, 5))
sns.barplot(x='Martial Status', y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Zbadajmy zawÃ³d oraz pÅ‚eÄ‡.

# In[44]:


plt.figure(figsize=(15, 5))
sns.barplot(x='Occupation', y="Target_cat", hue='Sex', data=train)
plt.xticks(rotation=90);


# Mam nadziejÄ™, Å¼e juÅ¼ widaÄ‡, ktÃ³re grupy wyrÃ³Å¼niajÄ… siÄ™ (a to brzmi jak cecha). Tu przy okazji zbadaliÅ›my temat niesprawiedliwoÅ›ci tego Å›wiata. Co jest waÅ¼ne, model nic nie wie na temat dyskryminacji, jedynie uczy siÄ™ z tego co jest "myÅ›lÄ…c", Å¼e to jest normalne. StÄ…d wÅ‚aÅ›nie pojawia siÄ™ bias, zobacz ten [filmik](https://www.youtube.com/watch?v=59bMh59JQDo). Musisz na to uwaÅ¼aÄ‡! Model staje siÄ™ tym, czym go karmisz :). Podobnie jak nasz mÃ³zg (to co tam wrzucamy, wpÅ‚ywa na to, kim siÄ™ stajemy pÃ³Åºniej).
# 
# 
# Zbadaj teraz jeszcze inne kombinacje np. zamiast pÅ‚ci sprawdÅº rasÄ™.

# In[45]:


plt.figure(figsize=(15, 5))
sns.barplot(x='Occupation', y="Target_cat", hue='Race', data=train)
plt.xticks(rotation=90);


# Zobacz jeszcze jednÄ… wskazÃ³wkÄ™ co do wizualizacji. MoÅ¼esz rozbiÄ‡ to na osobne wykresy.

# In[46]:


plt.figure(figsize=(20, 5))
g = sns.catplot(x="Occupation", y="Target_cat", col="Sex", data=train, kind="bar")

for ax in g.axes.flatten():
    plt.sca(ax)
    plt.xticks(rotation=90)


# JeÅ›li jest wiÄ™cej niÅ¼ 5, to moÅ¼na powiedzieÄ‡, ile ma byÄ‡ maksymalnie w jednym wierszu (uÅ¼ywajÄ…c `col_wrap`).

# In[47]:


plt.figure(figsize=(20, 5))
g = sns.catplot(x="Occupation", y="Target_cat", hue="Sex", col='Race', col_wrap=3, data=train, kind="bar")

for ax in g.axes.flatten():
    plt.sca(ax)
    plt.xticks(rotation=90)


# ZwrÃ³Ä‡ uwagÄ™ na czarne pionowe kreski (w sÅ‚upkach). Traktuj to jak rozrzut danych, jeÅ›li ta kreska jest zbyt zmienna (np. zobacz ostatni wykres), to ciÄ™Å¼ko cokolwiek wnioskowaÄ‡, bo jest zbyt duÅ¼a zmiennoÅ›Ä‡.
# 
# JuÅ¼ sporo wiesz, Å¼eby dalej poruszaÄ‡ siÄ™ samodzielnie. SprÃ³buj zrobiÄ‡ nastÄ™pujÄ…ce rzeczy.

# ## Zadanie 1.5.4
# Dodaj kolejne cechy (*features*) bazujÄ…c na tych, ktÃ³re juÅ¼ sÄ… (np. na podstawie dwÃ³ch cech `Relationship` i `Race`, moÅ¼na wykombinowaÄ‡ kilka rÃ³Å¼nych cech, np. `White` + `Husband` lub `Black` + `Husband`).
# 
# Moje oczekiwania sÄ… takie, Å¼e sprÃ³bujesz stworzyÄ‡ kilka czy kilkanaÅ›cie nowych cech. Od razu powiem, przygotuj siÄ™, Å¼e czÄ™sto nowa cecha moÅ¼e byÄ‡ maÅ‚o wartoÅ›ciowa. Natomiast wartoÅ›ciÄ… bÄ™dzie, jeÅ›li nauczysz siÄ™ szybko iterowaÄ‡ hipotezy (czyli odkrywaÄ‡ nowe cechy, ktÃ³re wnoszÄ… wartoÅ›Ä‡ poprzez szybkie eksperymenty).
# 
# Ciekawostka: byÄ‡ moÅ¼e warto dodaÄ‡ zmiennÄ… waga? Zobacz ten artykuÅ‚: [People who are overweight get paid less, according to a new LinkedIn study](https://bit.ly/39njuch)

# Daj znaÄ‡ na Slacku, czy udaÅ‚o Ci siÄ™ sprawdziÄ‡ tÄ™ cechÄ™ :) 

# In[48]:


train['Black_Husband'] = train['Race'].map(lambda y: int(y == 'Black')) * train['Relationship'].map(lambda z: int(z == 'Husband'))

plt.figure(figsize=(10,4))
sns.barplot(x='Black_Husband', y="Target", data=train)
plt.xticks(rotation=90)


# In[1]:


train["relationship_race"] = train.apply(lambda x: "{}-{}".format(x["Relationship"], x["Race"]), axis=1)
train["relationship_race_cat"] = train["relationship_race"].factorize()[0]


# ## Zadanie 1.5.5
# Zastosuj bardziej zÅ‚oÅ¼ony model, np. [DecisionTreeClassifier](https://bit.ly/39qD4Vk). Dlaczego akurat ten? Bo jest relatywnie prosty, ale znacznie skuteczniejszy niÅ¼ `Dummy` model. WiÄ™cej o drzewach decyzyjnych bÄ™dzie w nastÄ™pnych moduÅ‚ach. Dlatego na razie moÅ¼esz to potraktowaÄ‡ jako czarne pudÅ‚o.

# In[4]:


from sklearn.tree import DecisionTreeClassifier
def train_and_predict_model(X_train, X_test, y_train, y_test, model, success_metric=accurancy_score):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return success_metric(y_test, y_pred)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random.state=2018)
lr = train_and_predict_model(X_train, X_test, y_train, y_test, LogisticRegression())
print('LogisticRegression: ', lr)
print('')
dtc = train_and_predict_model(X_train, X_test, y_train, y_test, DecisionTreeClassifier())
print('DecisionTreeClassifier: ', dtc)
train.columns
